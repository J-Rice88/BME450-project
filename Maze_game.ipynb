{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "seed = 123\n",
    "\n",
    "## FUNCTION FROM CHAT GPT\n",
    "def create_maze(size, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    maze = [[2] * (size + 2) for _ in range(size + 2)]  # Initialize maze with all walls\n",
    "    \n",
    "    # Create an open space in the center\n",
    "    center = size // 2\n",
    "    maze[center + 1][center + 1] = 0\n",
    "    \n",
    "    # Recursive backtracking algorithm to generate the maze\n",
    "    def generate(x, y):\n",
    "        directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "        random.shuffle(directions)\n",
    "        \n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + 2*dx, y + 2*dy\n",
    "            if 0 < nx < size + 1 and 0 < ny < size + 1 and maze[ny][nx]:\n",
    "                maze[y + dy][x + dx] = 0\n",
    "                maze[ny][nx] = 0\n",
    "                generate(nx, ny)\n",
    "    \n",
    "    generate(center + 1, center + 1)\n",
    "    \n",
    "    # Surround the maze with walls\n",
    "    for i in range(size + 2):\n",
    "        maze[i][0] = 2\n",
    "        maze[i][-1] = 2\n",
    "        maze[0][i] = 2\n",
    "        maze[-1][i] = 2\n",
    "    \n",
    "    return maze\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for Stable Baseline 3 for the classic Snake \n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['console','rgb_array']}\n",
    "    maze_seed = 123\n",
    "    #Direction constants\n",
    "    n_actions = 4 #3 possible steps each turn\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    LEFT = 3\n",
    "    #Grid label constants\n",
    "    EMPTY = 0\n",
    "    PLAYER = 1\n",
    "    WALL = 2\n",
    "    KEY = 3\n",
    "    GOAL = 4\n",
    "    #Rewards\n",
    "    #REWARD_PER_STEP = 0 # reward for every step taken, gets into infinite loops if >0\n",
    "    #Define Max steps to avoid infinite loops\n",
    "    #should be lower than -REWARD_PER_STEP_TOWARDS_FOOD to avoid hitting wall intentionally\n",
    "    REWARD_PER_STEP_TOWARDS_GOAL = 3 #give reward for moving towards food and penalty for moving awa5\n",
    "    REWARD_PER_STEP_AWAY = -2\n",
    "    REWARD_PER_WALL_HIT = -1\n",
    "    REWARD_FOR_KEY = 100 \n",
    "    REWARD_FOR_GOAL = 200\n",
    "    MAX_STEPS = 500 #stop if we go too long without food to avoid infinite loops\n",
    "\n",
    "\n",
    "    def __init__(self, grid_size=12):\n",
    "        super(Maze, self).__init__()\n",
    "\n",
    "        self.stepnum = 0\n",
    "        # Size of the 2D grid (including walls)\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the snake\n",
    "        \n",
    "        #Init the grid\n",
    "        \n",
    "        self.grid = create_maze(self.grid_size - 2, self.maze_seed)\n",
    "        self.grid = np.array(self.grid, np.uint8)\n",
    "\n",
    "        #sets player location\n",
    "        self.player_location = (6,6)\n",
    "        self.grid[self.player_location[0]][self.player_location[1]] = 1\n",
    "\n",
    "        self.goal_location = (8,8)\n",
    "        self.grid[8][8] = 4\n",
    "\n",
    "        #empty_tiles = np.argwhere(self.grid==self.EMPTY)\n",
    "        #key_pos=empty_tiles[np.random.randint(0,len(empty_tiles))]\n",
    "        #self.grid[key_pos[0],key_pos[1]] = self.KEY\n",
    "\n",
    "        self.key_location = (4,6)\n",
    "        self.grid[4][6] = 3\n",
    "\n",
    "        self.key_status = 0\n",
    "        \n",
    "        #Init distance to food\n",
    "        self.player_dist_to_Key = self.grid_distance(self.grid, self.player_location, self.key_location)\n",
    "        self.player_dist_to_Goal = self.grid_distance(self.grid, self.player_location, self.goal_location)\n",
    "        #Store init values\n",
    "        self.init_grid = self.grid.copy()\n",
    "        self.init_player_location = self.player_location\n",
    "        self.init_goal_location = self.goal_location\n",
    "        self.init_key_location = self.key_location\n",
    "        self.init_key_status = self.key_status\n",
    "        \n",
    "        # The action space\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        # The observation space, \"position\" is the coordinates of the head; \"direction\" is which way the sanke is heading, \"grid\" contains the full grid info\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            spaces={\n",
    "                \"position\": gym.spaces.Box(low=0, high=(self.grid_size-1), shape=(2,), dtype=np.int32),\n",
    "                \"key\": gym.spaces.Box(low=0, high = 1, shape = (1,), dtype = np.int32),\n",
    "                #\"direction\": gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
    "                \"grid\": gym.spaces.Box(low = 0, high = 4, shape = (144,), dtype=np.uint8),\n",
    "            })\n",
    "        \n",
    "    def grid_distance(self, maze, start, end):\n",
    "        directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "        visited = set()\n",
    "        queue = deque([(start, 0)])  # Start position and distance\n",
    "\n",
    "        while queue:\n",
    "            ((x, y), distance) = queue.popleft()\n",
    "            if (x, y) == end:\n",
    "                return distance  # Return the distance when the end is reached\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < len(maze[0]) and 0 <= ny < len(maze) and maze[ny][nx] != 2 and (nx, ny) not in visited:\n",
    "                    visited.add((nx, ny))\n",
    "                    queue.append(((nx, ny), distance + 1))\n",
    "\n",
    "        return float('inf')  # If end is not reachable\n",
    "\n",
    "    \n",
    "    def reset(self, seed = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.stepnum = 0\n",
    "        self.grid = self.init_grid.copy()\n",
    "        self.player_location = self.init_player_location\n",
    "        self.key_location = self.init_goal_location\n",
    "        self.goal_location = self.init_key_location\n",
    "        self.key_status = self.init_key_status\n",
    "        #Init distance to food\n",
    "        self.player_dist_to_Key = self.grid_distance(self.grid,self.player_location, self.key_location)\n",
    "        self.player_dist_to_Goal = self.grid_distance(self.grid,self.player_location, self.goal_location)\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "            #direction = np.array(self.snake_coordinates[-1]) - np.array(self.snake_coordinates[-2])\n",
    "            #return observation in the format of self.observation_space\n",
    "            #key = torch.tensor(np.array(self.key_status, dtype = np.int32))\n",
    "            #grid = torch.tensor(self.grid)\n",
    "            return {\"position\": np.array(self.player_location,dtype=np.int32),\n",
    "                    \"key\": np.array(np.array(self.key_status, dtype = np.int32).flatten()),\n",
    "                    #\"direction\" : direction.astype(np.int32),\n",
    "                    \"grid\": np.array(self.grid.flatten(), dtype = np.uint8)\n",
    "                    }\n",
    "    def step(self,action):\n",
    "     \n",
    "        if action == self.UP:\n",
    "            step = (-1,0) \n",
    "        elif action == self.RIGHT:\n",
    "            step = (0,1)\n",
    "        elif action == self.LEFT:\n",
    "            step = (0,-1)\n",
    "        elif action == self.DOWN:\n",
    "            step = (1,0)\n",
    "        else:\n",
    "            raise ValueError(\"Action=%d is not part of the action space\"%(action))\n",
    "        #New head coordinate\n",
    "        new_coord = (np.array(self.player_location) + step).astype(np.int32)\n",
    "        #grow snake     \n",
    "        \n",
    "        #Check what is at the new position\n",
    "        new_pos = new_coord\n",
    "        new_pos_type = self.grid[new_pos[0]][new_pos[1]]\n",
    "        #self.grid[new_pos[0]][new_pos[1]] = self.PLAYER #this position is now occupied by the player\n",
    "        done = False; early = False; reward = 0 #by default the game goes on and no reward   \n",
    "        if new_pos_type == self.KEY:\n",
    "            reward += self.REWARD_FOR_KEY\n",
    "            self.key_status = 1\n",
    "            self.grid[self.player_location[0],self.player_location[1]] = self.EMPTY\n",
    "            self.grid[new_pos[0]][new_pos[1]] = self.PLAYER\n",
    "            self.player_location = new_pos\n",
    "            #Put down a new food item\n",
    "        elif new_pos_type == self.GOAL:\n",
    "            reward += self.REWARD_FOR_GOAL\n",
    "            reward += (self.MAX_STEPS / 2) - self.stepnum\n",
    "            self.grid[self.player_location[0],self.player_location[1]] = self.EMPTY\n",
    "            self.grid[new_pos[0]][new_pos[1]] = self.PLAYER\n",
    "            self.player_location = new_pos\n",
    "            done = True\n",
    "        elif new_pos_type == self.WALL:\n",
    "            new_pos = self.player_location\n",
    "            reward += self.REWARD_PER_WALL_HIT\n",
    "        else:\n",
    "            self.grid[self.player_location[0],self.player_location[1]] = self.EMPTY\n",
    "            self.grid[new_pos[0]][new_pos[1]] = self.PLAYER\n",
    "            self.player_location = new_pos\n",
    "    \n",
    "            \n",
    "        \n",
    "        if self.key_status == 0:\n",
    "            prev_dist = self.player_dist_to_Key\n",
    "            self.player_dist_to_Key = self.grid_distance(self.grid,self.player_location, self.key_location)\n",
    "            current_dist = self.player_dist_to_Key\n",
    "        else:\n",
    "            prev_dist = self.player_dist_to_Goal\n",
    "            self.player_dist_to_Goal = self.grid_distance(self.grid,self.player_location, self.key_location)\n",
    "            current_dist = self.player_dist_to_Goal\n",
    "        \n",
    "        if current_dist < prev_dist:\n",
    "            reward += self.REWARD_PER_STEP_TOWARDS_GOAL\n",
    "        if current_dist > prev_dist:\n",
    "            reward += self.REWARD_PER_STEP_AWAY\n",
    "        \n",
    "        if self.stepnum >= self.MAX_STEPS:\n",
    "            early = True\n",
    "        self.stepnum += 1\n",
    "\n",
    "\n",
    "        return  self._get_obs(), reward, done, early, {}    \n",
    "                 \n",
    "    def render(self, mode='rgb_array'):\n",
    "        if mode == 'console':\n",
    "            print(self.grid)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.maze_plot()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def maze_plot(self, plot_inline=False):\n",
    "        wall_ind = (self.grid==self.WALL)\n",
    "        player_ind = (self.grid==self.PLAYER)\n",
    "        key_ind = (self.grid==self.KEY)\n",
    "        goal_ind = (self.grid == self.GOAL)\n",
    "        #Create color array for plot, default white color\n",
    "        Color_array=np.zeros((self.grid_size,self.grid_size,3),dtype=np.uint8)+255 #default white\n",
    "        Color_array[wall_ind,:]= np.array([0,0,0]) #black walls\n",
    "        Color_array[player_ind,:]= np.array([255,0,0]) #redish snake\n",
    "        Color_array[goal_ind,:]= np.array([0,255,0]) #green goal \n",
    "        Color_array[key_ind,:]= np.array([255,255,0]) #yellow key \n",
    "        #plot\n",
    "        if plot_inline:\n",
    "            fig=plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(Color_array, interpolation='nearest')\n",
    "            plt.show()\n",
    "        return Color_array\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built in environment check\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = Maze()\n",
    "# If the environment doesn't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "#Logging\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Instantiate the env\n",
    "env = Maze()\n",
    "# wrap it\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "\n",
    "#Callback, this built-in function will periodically evaluate the model and save the best version\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./log/',\n",
    "                             log_path='./log/', eval_freq=5000,\n",
    "                             deterministic=False, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=490.00 +/- 2.19\n",
      "Episode length: 49.00 +/- 1.10\n",
      "New best mean reward!\n",
      "Calculation took 0 hr 0 min 15.0064 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "#Train the agent\n",
    "max_total_step_num = 5000\n",
    "\n",
    "def learning_rate_schedule(progress_remaining):\n",
    "    start_rate = 0.0001 #0.0003\n",
    "    #Can do more complicated ones like below\n",
    "    #stepnum = max_total_step_num*(1-progress_remaining)\n",
    "    #return 0.003 * np.piecewise(stepnum, [stepnum>=0, stepnum>4e4, stepnum>2e5, stepnum>3e5], [1.0,0.5,0.25,0.125 ])\n",
    "    return start_rate * progress_remaining #linearly decreasing\n",
    "\n",
    "PPO_model_args = {\n",
    "    \"learning_rate\": learning_rate_schedule, #decreasing learning rate #0.0003 #can be set to constant\n",
    "    \"gamma\": 0.99, #0.99, discount factor for futurer rewards, between 0 (only immediate reward matters) and 1 (future reward equivalent to immediate), \n",
    "    \"verbose\": 0, #change to 1 to get more info on training steps\n",
    "    #\"seed\": 137, #fixing the random seed\n",
    "    \"ent_coef\": 0, #0, entropy coefficient, to encourage exploration\n",
    "    \"clip_range\": 0.2 #0.2, very roughly: probability of an action can not change by more than a factor 1+clip_range\n",
    "}\n",
    "starttime = time.time()\n",
    "model = PPO('MultiInputPolicy', env,**PPO_model_args)\n",
    "#Load previous best model parameters, we start from that\n",
    "if os.path.exists(\"log/best_model.zip\"):\n",
    "    model.set_parameters(\"log/best_model.zip\")\n",
    "model.learn(max_total_step_num,callback=eval_callback)\n",
    "dt = time.time()-starttime\n",
    "print(\"Calculation took %g hr %g min %g s\"%(dt//3600, (dt//60)%60, dt%60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "# Helper from the library, a bit hard to read but immediately useable\n",
    "results_plotter.plot_results([\"log\"], 1e7, results_plotter.X_TIMESTEPS,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's reward: 492 +/-   0\n"
     ]
    }
   ],
   "source": [
    "#Load back the best model\n",
    "model.set_parameters(\"log/best_model.zip\")\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Evaluate the trained model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "print(\"Best model's reward: %3.3g +/- %3.3g\"%(mean_reward,std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Action:  1 Tot. Reward: 3\n",
      "Step 2 Action:  1 Tot. Reward: 6\n",
      "Step 3 Action:  1 Tot. Reward: 9\n",
      "Step 4 Action:  1 Tot. Reward: 12\n",
      "Step 5 Action:  3 Tot. Reward: 10\n",
      "Step 6 Action:  3 Tot. Reward: 8\n",
      "Step 7 Action:  3 Tot. Reward: 6\n",
      "Step 8 Action:  3 Tot. Reward: 4\n",
      "Step 9 Action:  0 Tot. Reward: 2\n",
      "Step 10 Action:  0 Tot. Reward: 0\n",
      "Step 11 Action:  0 Tot. Reward: -2\n",
      "Step 12 Action:  0 Tot. Reward: -4\n",
      "Step 13 Action:  2 Tot. Reward: -1\n",
      "Step 14 Action:  2 Tot. Reward: 2\n",
      "Step 15 Action:  0 Tot. Reward: 0\n",
      "Step 16 Action:  0 Tot. Reward: -2\n",
      "Step 17 Action:  3 Tot. Reward: 1\n",
      "Step 18 Action:  3 Tot. Reward: 4\n",
      "Step 19 Action:  0 Tot. Reward: 2\n",
      "Step 20 Action:  0 Tot. Reward: 0\n",
      "Step 21 Action:  2 Tot. Reward: -2\n",
      "Step 22 Action:  2 Tot. Reward: -4\n",
      "Step 23 Action:  2 Tot. Reward: -6\n",
      "Step 24 Action:  2 Tot. Reward: -8\n",
      "Step 25 Action:  2 Tot. Reward: -10\n",
      "Step 26 Action:  2 Tot. Reward: -12\n",
      "Step 27 Action:  2 Tot. Reward: -14\n",
      "Step 28 Action:  2 Tot. Reward: -16\n",
      "Step 29 Action:  1 Tot. Reward: -18\n",
      "Step 30 Action:  1 Tot. Reward: -20\n",
      "Step 31 Action:  1 Tot. Reward: -21\n",
      "Step 32 Action:  3 Tot. Reward: -18\n",
      "Step 33 Action:  3 Tot. Reward: -15\n",
      "Step 34 Action:  3 Tot. Reward: -12\n",
      "Step 35 Action:  3 Tot. Reward: 86\n",
      "Step 36 Action:  2 Tot. Reward: 84\n",
      "Step 37 Action:  2 Tot. Reward: 82\n",
      "Step 38 Action:  1 Tot. Reward: 80\n",
      "Step 39 Action:  1 Tot. Reward: 78\n",
      "Step 40 Action:  2 Tot. Reward: 81\n",
      "Step 41 Action:  2 Tot. Reward: 84\n",
      "Step 42 Action:  1 Tot. Reward: 87\n",
      "Step 43 Action:  1 Tot. Reward: 90\n",
      "Step 44 Action:  1 Tot. Reward: 88\n",
      "Step 45 Action:  1 Tot. Reward: 86\n",
      "Step 46 Action:  3 Tot. Reward: 84\n",
      "Step 47 Action:  3 Tot. Reward: 82\n",
      "Step 48 Action:  0 Tot. Reward: 85\n",
      "Step 49 Action:  0 Tot. Reward: 490\n",
      "Game over! tot. reward= 490.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the trained agent and save animation\n",
    "obs, info = env.reset()\n",
    "#Framework to save animgif\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "frames = []\n",
    "fps=18\n",
    "\n",
    "n_steps = 500\n",
    "tot_reward = 0\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, truncate, info = env.step(action)\n",
    "    tot_reward += reward\n",
    "    print(\"Step {}\".format(step + 1),\"Action: \", action, 'Tot. Reward: %g'%(tot_reward))\n",
    "    #print('position=', obs['position'], 'direction=', obs['direction'])\n",
    "    #env.render(mode='console')\n",
    "    frames.append([ax.imshow(env.render(), animated=True)])\n",
    "    if done:\n",
    "        print(\"Game over!\", \"tot. reward=\", tot_reward)\n",
    "        break\n",
    "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None) #to remove white bounding box \n",
    "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True,repeat_delay=1000)\n",
    "anim.save(\"snake_best.gif\",dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING BLOCK\n",
    "\n",
    "x = (np.array((6,6)) + (1,0)).astype(np.int32)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
